import yfinance as yf
import pandas as pd
from valuetracker.ratios import get_ratios
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import os

def fetch_ticker_data(row):
    """
    PomocnÃ¡ funkce: stÃ¡hne data pro jeden ticker.
    """
    ticker = row['Symbol']
    name = row['Name']
    try:
        stock = yf.Ticker(ticker)
        info = stock.info
        financials = stock.financials
        balance_sheet = stock.balance_sheet
        ratios = get_ratios(info, financials, balance_sheet)
        ratios['Ticker'] = ticker
        ratios['Name'] = name
        ratios['Sector'] = info.get('sector')
        print(f"âœ… {ticker} staÅ¾eno")
        return ratios
    except Exception as e:
        print(f"âŒ {ticker} chyba: {e}")
        return None

import yfinance as yf
import pandas as pd
import time
import os

def download_sector_data(sp500_df, csv_path="sector_ratios.csv", batch_size=50, max_retries=5):
    """
    ğŸ“¥ StÃ¡hne / aktualizuje closing ceny pro vÅ¡echny firmy v S&P 500 po dÃ¡vkÃ¡ch.
    
    âœ… FUNKCE:
    - PÅ™i prvnÃ­m spuÅ¡tÄ›nÃ­ stÃ¡hne celou historii (period='max')
    - PÅ™i dalÅ¡Ã­ch spuÅ¡tÄ›nÃ­ch stÃ¡hne jen poslednÃ­ch 5 dnÃ­ (rychlejÅ¡Ã­)
    - PouÅ¾Ã­vÃ¡ batch stahovÃ¡nÃ­ (defaultnÄ› 50 tickerÅ¯ najednou)
    - Exponential backoff pÅ™i Too Many Requests
    - UklÃ¡dÃ¡nÃ­ checkpointÅ¯ po kaÅ¾dÃ©m batchi â†’ kdyÅ¾ notebook spadne, data nezmizÃ­
    - OdstraÅˆuje duplicity podle `Ticker` + `Date`
    
    âš™ï¸ PARAMETRY:
    - sp500_df: DataFrame se sloupci ['Symbol', 'Name']
    - csv_path: cesta k CSV (defaultnÄ› 'sector_ratios.csv')
    - batch_size: kolik tickerÅ¯ stahovat najednou (defaultnÄ› 50)
    - max_retries: kolikrÃ¡t zkusit batch pÅ™i chybÄ› (defaultnÄ› 5)
    """

    # âœ… ZkusÃ­me naÄÃ­st existujÃ­cÃ­ CSV (pokud existuje)
    if os.path.exists(csv_path):
        existing_df = pd.read_csv(csv_path)
        print(f"âœ… NaÄteno {len(existing_df)} Å™Ã¡dkÅ¯ z {csv_path}. Aktualizuji novÃ¡ dataâ€¦")
        csv_exists = True
    else:
        print(f"âš ï¸ Soubor {csv_path} neexistuje â€“ provÃ¡dÃ­m prvnÃ­ kompletnÃ­ staÅ¾enÃ­ vÅ¡ech dat.")
        existing_df = pd.DataFrame()
        csv_exists = False

    tickers = sp500_df['Symbol'].tolist()
    all_data = []

    # âœ… RozdÄ›lÃ­me tickery na dÃ¡vky
    for i in range(0, len(tickers), batch_size):
        batch = tickers[i:i+batch_size]
        print(f"\nğŸ“¦ Batch {i//batch_size + 1}/{len(tickers)//batch_size + 1}: {batch}")

        # ğŸ†• KdyÅ¾ CSV uÅ¾ existuje, stÃ¡hneme jen poslednÃ­ch 5 dnÃ­
        period = "5d" if csv_exists else "max"

        # ğŸ” Retry s exponential backoff
        retries = 0
        while retries < max_retries:
            try:
                df_batch = yf.download(batch, period=period, group_by='ticker', progress=False)
                if df_batch.empty:
                    raise ValueError("Batch nevrÃ¡til Å¾Ã¡dnÃ¡ data.")
                
                # âœ… Zpracujeme vÃ½sledky
                for ticker in batch:
                    try:
                        ticker_df = df_batch[ticker].copy()
                        ticker_df = ticker_df.reset_index()
                        ticker_df = ticker_df[['Date', 'Close']]
                        ticker_df['Ticker'] = ticker
                        all_data.append(ticker_df)
                        print(f"âœ… {ticker} staÅ¾eno ({len(ticker_df)} Å™Ã¡dkÅ¯)")
                    except Exception:
                        # âš ï¸ NÄ›kterÃ© tickery vrÃ¡tÃ­ prÃ¡zdnÃ¡ data
                        print(f"âš ï¸ Pro {ticker} nebyla nalezena data.")

                # ğŸ’¾ âœ… Checkpoint â€“ prÅ¯bÄ›Å¾nÄ› uklÃ¡dÃ¡me
                if all_data:
                    df_checkpoint = pd.concat([existing_df] + all_data, ignore_index=True)
                    df_checkpoint.drop_duplicates(subset=["Ticker", "Date"], keep="last", inplace=True)
                    df_checkpoint.to_csv(csv_path, index=False)
                    print(f"ğŸ’¾ Checkpoint uloÅ¾en ({len(df_checkpoint)} Å™Ã¡dkÅ¯).")

                # ğŸ’¤ KrÃ¡tkÃ½ spÃ¡nek, aby se snÃ­Å¾ila zÃ¡tÄ›Å¾ API
                time.sleep(2)
                break  # âœ… batch hotovÃ½, vypadneme z retry loopu

            except Exception as e:
                retries += 1
                wait_time = 5 * (2 ** retries)  # exponential backoff: 10, 20, 40, ...
                print(f"âŒ Chyba pÅ™i stahovÃ¡nÃ­ batch {i//batch_size + 1}: {e}")
                if retries < max_retries:
                    print(f"â³ ÄŒekÃ¡m {wait_time}s a zkusÃ­m to znovu (pokusu {retries}/{max_retries})â€¦")
                    time.sleep(wait_time)
                else:
                    print(f"ğŸš¨ Batch {i//batch_size + 1} pÅ™eskoÄen po {max_retries} pokusech.")
                    break

    if not all_data:
        print("âŒ Nebyla zÃ­skÃ¡na Å¾Ã¡dnÃ¡ novÃ¡ data.")
        return

    # âœ… SpojÃ­me vÅ¡echny dÃ¡vky
    df_new = pd.concat(all_data, ignore_index=True)

    # âœ… SpojÃ­me s existujÃ­cÃ­m CSV a odstranÃ­me duplicity
    if not existing_df.empty:
        combined = pd.concat([existing_df, df_new], ignore_index=True)
        combined.drop_duplicates(subset=["Ticker", "Date"], keep="last", inplace=True)
    else:
        combined = df_new

    # âœ… UloÅ¾Ã­me zpÄ›t do CSV
    combined.to_csv(csv_path, index=False)
    print(f"\nğŸ“‚ FinÃ¡lnÃ­ uloÅ¾enÃ­ hotovo! CSV obsahuje {len(combined)} Å™Ã¡dkÅ¯.")

